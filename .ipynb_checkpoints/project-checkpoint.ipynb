{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c917f496-e9b4-4fb8-97ca-81b0e672a967",
     "showTitle": false,
     "title": ""
    },
    "id": "WRVr5-MqggZ-"
   },
   "source": [
    "# Video Game Playability Analysis Based on Players’ Reviews with PySpark\n",
    "\n",
    "## Big Data Computing final project - A.Y. 2022-2023\n",
    "\n",
    "Prof. Gabriele Tolomei\n",
    "\n",
    "MSc in Computer Science\n",
    "\n",
    "La Sapienza, University of Rome\n",
    "\n",
    "### Author\n",
    "\n",
    "Ilaria De Sio - [desio.2064970@studenti.uniroma1.it](mailto:desio.2064970@studenti.uniroma1.it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c9c61d08-6af9-4265-8dfc-8082bb9e4ae8",
     "showTitle": false,
     "title": ""
    },
    "id": "gSMieUC0ggaB"
   },
   "source": [
    "The project is based on the paper entitled *A Data-Driven Approach for Video Game\n",
    "Playability Analysis Based on Players’ Reviews* in this case study, the definition of\n",
    "playability analyzed consists of three basic concepts ”**functionality**, **usability**, and\n",
    "**gameplay**” defined by the *framework of Paavilainen*.\n",
    "\n",
    "The goal is to obtain an explicit\n",
    "and simplified framework so that not only the intuitively quantified assessment of the\n",
    "overall playability of the chosen game is obtained but also to analyze and be able\n",
    "to view the positive and negative aspects of it, and while classifying the information\n",
    "that can be ”playability-informative” and ”non-playability-informative” divided into\n",
    "the classes listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81a78200-eeac-4e26-9afa-cb556751b238",
     "showTitle": false,
     "title": ""
    },
    "id": "mw8uGOrDggaB"
   },
   "source": [
    "## Define some global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T14:28:27.005200Z",
     "start_time": "2023-06-20T14:28:27.000939Z"
    },
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c02dea2-d3da-4f60-a576-1d63e49a11e0",
     "showTitle": false,
     "title": ""
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1687268429135,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "0Qvq62hYggaC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c8ba324d-2cdc-4c80-9a1a-afbfd5939212",
     "showTitle": false,
     "title": ""
    },
    "id": "ENWZUOLQggaE"
   },
   "source": [
    "## Import PySpark packages and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-20T14:28:27.004150Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "executionInfo": {
     "elapsed": 63614,
     "status": "error",
     "timestamp": 1687270963679,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "Gh2ZI4qzjvTS",
    "is_executing": true,
    "outputId": "67e78a26-d2c4-4821-c231-48e1567c9ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/ilariadesio/opt/anaconda3/lib/python3.9/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /Users/ilariadesio/opt/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.5)\n",
      "zsh:cd:1: no such file or directory: /Users/ilariadesio/.ivy2/cache/com.johnsnowlabs.nlp/spark-nlp_2.12/jars\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pt/3cv5yjcs1rv1hdw6xwxdchz00000gn/T/ipykernel_22182/2061433849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResourceDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mResourceDownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowPublicPipelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sparknlp/pretrained/resource_downloader.py\u001b[0m in \u001b[0;36mshowPublicPipelines\u001b[0;34m(lang, version)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mVersion\u001b[0m \u001b[0mof\u001b[0m \u001b[0mSpark\u001b[0m \u001b[0mNLP\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ShowPublicPipelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sparknlp/internal/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, version)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_ShowPublicPipelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtendedJavaWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         super(_ShowPublicPipelines, self).__init__(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \"com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.showPublicPipelines\", lang, version)\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sparknlp/internal/extended_java_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtendedJavaWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sparknlp/internal/extended_java_wrapper.py\u001b[0m in \u001b[0;36mnew_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpylist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n\u001b[1;32m     79\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install -q pyspark==3.3.0 spark-nlp==4.3.2\n",
    "\n",
    "! cd ~/.ivy2/cache/com.johnsnowlabs.nlp/spark-nlp_2.12/jars && ls -lt\n",
    "\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "ResourceDownloader.showPublicPipelines(lang=\"en\")\n",
    "\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ad2abf6-dae0-492a-ae76-bf3074217bd3",
     "showTitle": false,
     "title": ""
    },
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1687269721934,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "Un5u3ObQggaE",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import sparknlp\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import *\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14982,
     "status": "ok",
     "timestamp": 1687268498873,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "6jnX0f-jHyfX",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Create the session\n",
    "conf = SparkConf().\\\n",
    "                set('spark.ui.port', \"4050\").\\\n",
    "                set('spark.executor.memory', '4G').\\\n",
    "                set('spark.driver.memory', '45G').\\\n",
    "                set('spark.driver.maxResultSize', '10G').\\\n",
    "                setAppName(\"PySparkTutorial\").\\\n",
    "                setMaster(\"local[*]\")\n",
    "\n",
    "# Create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "50e546d7-be32-44d2-bb03-b9012ae7322a",
     "showTitle": false,
     "title": ""
    },
    "id": "Hr5wlq6mggaF"
   },
   "source": [
    "## 1.  Dataset initialization\n",
    "I chose to use the dataset [https://doi.org/10.6084/m9.figshare.14222531.v1](https://doi.org/10.6084/m9.figshare.14222531.v1) directly provided by the authors of the paper containing the review data from Steam for **No Man’s Sky** in terms of playability by users.\n",
    "This case of study is really interesting because this game was released on 2016, before which a social media “hype” had been evoked leading to an unprecedentedly high expectation.\n",
    "Unexpectedly the release was disastrous, but for the last four years, the\n",
    "game has been continuously maintained with its quality gradually increasing, which makes it a unique case where the changes in game quality is observable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22243,
     "status": "ok",
     "timestamp": 1687268521107,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "G8sr_VdgI-5i",
    "is_executing": true,
    "outputId": "fefc2cb0-cec9-4a67-b040-d79c99d3d8a4"
   },
   "outputs": [],
   "source": [
    "PATH=\"/Users/ilariadesio/Desktop/Computerscience/Firstyear/Secondsemester/BigData/Projects/Video_Game_Playability_Analysis/input/data_clean.csv\"\n",
    "game_dataset = spark.read.load(PATH,\n",
    "                               format=\"csv\",\n",
    "                               sep=\",\",\n",
    "                               inferSchema=\"true\",\n",
    "                               header=\"true\")\n",
    "game_dataset = pd.read_csv(\n",
    "    \"/Users/ilariadesio/Desktop/Computerscience/Firstyear/Secondsemester/BigData/Projects/Video_Game_Playability_Analysis/input/data_clean.csv\")\n",
    "game_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709
    },
    "executionInfo": {
     "elapsed": 2331,
     "status": "ok",
     "timestamp": 1687268523433,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "2f8S4zKMJMgr",
    "is_executing": true,
    "outputId": "98e22d83-4129-494b-8a3e-b6d79ad526f7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2bbccd4-93b8-4565-9670-16ad7820d01a",
     "showTitle": false,
     "title": ""
    },
    "id": "CTT3vraEggaF"
   },
   "source": [
    "## 1.1 Dataset Shape and Scheme\n",
    "\n",
    "The dataset contains approximately 99k records of Steam's reviews.\n",
    "\n",
    "\n",
    "* ```recommendationid```: The review ID;\n",
    "* ```language```: Review language;\n",
    "* ```review```: The text of user review;\n",
    "* ```timestamp_created ```: The date a review is posted;\n",
    "* ```timestamp_updated```: Update date of a review;\n",
    "* ```voted_up```: True means it was a positive recommendation;\n",
    "* ```votes_up```: The number of other users who found this review helpful;\n",
    "* ```votes_funny```: How many other player think the review is funny;\n",
    "* ```weighted_cote_score```: Helpfulness score;\n",
    "* ```comment_count```: How many other player comment the review;\n",
    "* ```steam_purchase```: Game purchased on steam or not;\n",
    "* ```received_for_free```: Game received for free or not;\n",
    "* ```written_during_early_access```:\n",
    "* ```author_num_games_owned```: Number of games owned by the author;\n",
    "* ```author_num_reviews```: How many other reviews has this user done;\n",
    "* ```author_playtime_forever```: Number of total hours played by the author;\n",
    "* ```author_playtime_last_two_weeks```: Number of hours played by the author in the last two weeks;\n",
    "* ```author_last_played```:\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66QN3feK8nE9"
   },
   "source": [
    "Initially in this more visual phase the dataframe provided by pandas will be used, later in text processing and for the rest of the project it will fall back to the spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1687268523434,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "HqHrstZdJdO_",
    "is_executing": true,
    "outputId": "62b25303-8ec3-4285-b11f-14e67d38b3dd"
   },
   "outputs": [],
   "source": [
    "print(type(game_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2DcATuHj3wl"
   },
   "source": [
    "# 2. Data Pre-processing\n",
    "In this phase involves cleaning and transforming the raw data to ensure its quality and compatibility with the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7HFRcszn9qk"
   },
   "source": [
    "Convert to datetime the columns ```timestamp_created``` and ```timestamp_updated```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrLlA6yhAuYF"
   },
   "source": [
    "## 2.1 Data Cleaning\n",
    "\n",
    "From the data info above, we can already notice that there are missing values in review. Since our work is going to be heavily relying on this column, we have to clean it from these missing values. In addition, we also need to check for duplicated values following the standard data cleaning procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1687268523434,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "KxRyJ1FkYyCL",
    "is_executing": true,
    "outputId": "f45ee78a-cb23-4e57-b132-82c27883a615"
   },
   "outputs": [],
   "source": [
    "game_dataset[game_dataset['review'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1687268523435,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "k1CJGmLVZNKB",
    "is_executing": true,
    "outputId": "f5fc439b-7bba-4fa7-bb0e-935c197ddfa9"
   },
   "outputs": [],
   "source": [
    "game_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1687268523837,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "iZN5xnLEZqnm",
    "is_executing": true,
    "outputId": "8a9223f7-11d2-487b-e0d8-ec710672ac31"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing reviews\n",
    "game_dataset.dropna(inplace=True)\n",
    "\n",
    "# Sanity check\n",
    "game_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1687268523837,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "cdatonw6xuuQ",
    "is_executing": true,
    "outputId": "c191fc4c-5e4b-401f-f0bb-ab3801379755"
   },
   "outputs": [],
   "source": [
    "game_dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VydHh8Z8x2RI"
   },
   "source": [
    "Rows with null values have been deleted correctly, now the rows are 99957.\n",
    "Now let's check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1687268523838,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "sqxXm1V4Z1vz",
    "is_executing": true,
    "outputId": "302a55da-28b4-4041-b688-f33737651a89"
   },
   "outputs": [],
   "source": [
    "game_dataset.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVAT5Q2EaNFD"
   },
   "source": [
    "It seems that there are no duplicated rows. But are there duplicated reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1687268524445,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "bOhB4w3zaO4e",
    "is_executing": true,
    "outputId": "2a485cf8-8e00-454f-f56e-b69d3947124c"
   },
   "outputs": [],
   "source": [
    "game_dataset.duplicated(subset='review').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1687268524446,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "J562ceGbafCC",
    "is_executing": true,
    "outputId": "441638fc-b111-489f-d3b1-d61196c5a17b"
   },
   "outputs": [],
   "source": [
    "game_dataset[game_dataset.duplicated(subset='review',keep=False)].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u77e_xUzj3q"
   },
   "source": [
    "As we can see there are not actually equal reviews but with similar terms, most of them are very short reviews such as 'good' or 'amazing'. These reviews are still important for our classification task, so we will not drop them.\n",
    "\n",
    "##Text-processing\n",
    "We may note that some reviews may also be written only by special characters, these types of reviews should be removed, because there may be smilies or special characters are not significant and also that may have multiple or ambiguous meanings, making accurate interpretation difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGXW-f_I6lS4"
   },
   "source": [
    "###Convert all the text of the review to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17306,
     "status": "ok",
     "timestamp": 1687268541747,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "rMX15z9IcRhh",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "spark_df = spark.createDataFrame(game_dataset)\n",
    "\n",
    "convert_to_lowercase_udf = udf(convert_to_lowercase, StringType())\n",
    "\n",
    "# Application of the UDF to the 'review' column of the DataFrame\n",
    "spark_df = spark_df.withColumn('review_lower', convert_to_lowercase_udf(spark_df['review']))\n",
    "\n",
    "\"\"\"\n",
    "Since it is not possible to directly assign the UDF result to the same input column in Spark (this is due to the fact that Spark's DataFrames are immutable,\n",
    "which means that they cannot be changed directly),\n",
    "a new column called lower_reviews will be created and then replaced with the original reviews and deleted.\n",
    "\"\"\"\n",
    "spark_df = spark_df.withColumn(spark_df.columns[2], col('review_lower'))\n",
    "spark_df=spark_df.drop('review_lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfwE40zfTKdD"
   },
   "source": [
    "##Removal of extra-spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1687268541747,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "gdA3MUisTG0f",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1687268541748,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "ojdkt09FUnSJ",
    "is_executing": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjZK02GF-KMg"
   },
   "source": [
    "###Removal of non-ASCII characters\n",
    "For example, the dollar sign ($), accented letters such as à, é, ô, also there are many special symbols such as ☺ (smiley face) and others that are not included in standard ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9942,
     "status": "ok",
     "timestamp": 1687268551686,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "7naKQxAy-Wjq",
    "is_executing": true,
    "outputId": "82b8dbda-b471-4a4c-a8ed-b661044328dd"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    # Utilizziamo un'espressione regolare per trovare tutti i caratteri non ASCII\n",
    "    non_ascii_regex = re.compile('[^\\x00-\\x7F]')\n",
    "    # Sostituiamo i caratteri non ASCII con una stringa vuota\n",
    "    cleaned_text = non_ascii_regex.sub('', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Definizione della funzione UDF (User-Defined Function)\n",
    "remove_non_ascii_udf = udf(remove_non_ascii, StringType())\n",
    "\n",
    "# Applicazione della funzione UDF alla colonna 'review'\n",
    "spark_df = spark_df.withColumn('cleaned_review', remove_non_ascii_udf(spark_df['review']))\n",
    "\n",
    "spark_df = spark_df.withColumn(spark_df.columns[2], col('cleaned_review'))\n",
    "spark_df=spark_df.drop('cleaned_review')\n",
    "\n",
    "\n",
    "first_row = spark_df.select('review').show(n=5, truncate=False)\n",
    "print(first_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YKjcPjPdAgU"
   },
   "source": [
    "###Sentence Tokenization\n",
    "In this phase divide each review item from the DataFrame into sentence-level review instances, due to the fact that each review with multiple sentences can contain multiple topics and various sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5132,
     "status": "ok",
     "timestamp": 1687268556811,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "KsmCp3b-BOhS",
    "is_executing": true,
    "outputId": "79ea264c-fd52-4273-bcfa-80254e7ecc90"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "def sent_tokenize(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "# Creating the UDF function for the phrase tokenizer.\n",
    "sent_tokenize_udf = udf(sent_tokenize, ArrayType(StringType()))\n",
    "\n",
    "# Applying the phrase tokenizer to the 'review' column of the DataFrame.\n",
    "spark_df = spark_df.withColumn('sentences', sent_tokenize_udf(spark_df['review']))\n",
    "\n",
    "columns = spark_df.columns\n",
    "columns.remove('sentences')\n",
    "spark_df = spark_df.select(columns[:3] + ['sentences'] + columns[3:])\n",
    "\n",
    "first_row = spark_df.select('sentences').show(n=5, truncate=False)\n",
    "print(first_row)\n",
    "\n",
    "spark_df.printSchema();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFrIr8v5wVH6"
   },
   "source": [
    "### Remove Stop-words (NOT USED by Project Choice)\n",
    "I initially tried to apply to the removal of stop words, but seeing the results, I noticed that it might result in the loss of some relevant information about sentence structure, so I decided not to use it in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1687268556812,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "Rk4ADxB8K5Xn",
    "is_executing": true,
    "outputId": "d4340e12-6f48-4310-8b39-b48b12bc8b29"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_eng = stopwords.words('english')\n",
    "\n",
    "# Define a UDF function to remove stop-words.\n",
    "def remove_stopwords(sentence):\n",
    "    if sentence is not None:\n",
    "        return [word for word in sentence if word.lower() not in stopwords_eng]\n",
    "    return None\n",
    "\n",
    "remove_stopwords_udf = udf(remove_stopwords, ArrayType(StringType()))\n",
    "\n",
    "# Apply stop-word screening to the DataFrame.\n",
    "spark_df = spark_df.withColumn('filtered_sentences', remove_stopwords_udf(spark_df['sentences']))\n",
    "\n",
    "first_row = spark_df.select('filtered_sentences').show(n=5, truncate=False)\n",
    "print(first_row)\n",
    "spark_df.printSchema();\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgz7UnxjOFbz"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "error",
     "timestamp": 1687269959665,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "SZgCUScVIFtc",
    "is_executing": true,
    "outputId": "dd220f9b-54c5-449c-dfe2-674690360acd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "error",
     "timestamp": 1687269842606,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "f3pKHt9mStWZ",
    "is_executing": true,
    "outputId": "4e2d7f58-5262-4465-c489-35ad5cca1a66"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "\n",
    "# Creazione degli annotatori Spark NLP\n",
    "document_assembler = DocumentAssembler().setInputCol(\"review\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols([\"token\"]).setOutputCol(\"lemma\")\n",
    "\n",
    "# Creazione del pipeline\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, lemmatizer])\n",
    "\n",
    "# Applicazione del pipeline al DataFrame\n",
    "processed_df = pipeline.fit(spark_df).transform(spark_df)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "processed_df.select(\"review\", \"lemma.result\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN_zvztdoz4u"
   },
   "source": [
    "## 2.2 Data Exploration\n",
    "At this phase I will analyze different hypotheses of correlation tar the variables to actually test whether or not they are correlated according to the hypothesis provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLeUlIM3rJmt"
   },
   "source": [
    "### 2.2.1 First Hypothesis\n",
    "**Does there exist a correlation between the number of hours a person played a game and the sentiment of the review?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1687268556813,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "18UwZtI8pmbu",
    "is_executing": true,
    "outputId": "8dc03f5f-8fa1-40d0-e8c2-9a5a3bba6f5e"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "executionInfo": {
     "elapsed": 204700,
     "status": "error",
     "timestamp": 1687268761505,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "-CBndJErkWj6",
    "is_executing": true,
    "outputId": "137f1347-b1b7-4536-fc35-4af69b41496c"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Converte il DataFrame Pandas in un DataFrame PySpark\n",
    "spark_df = spark.createDataFrame(game_dataset)\n",
    "\n",
    "# Inizializza il SentimentIntensityAnalyzer di NLTK\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Definisci la funzione per l'analisi del sentiment\n",
    "def analyze_sentiment(review):\n",
    "    # Calcola il sentiment della recensione utilizzando il SentimentIntensityAnalyzer di NLTK\n",
    "    sentiment = sia.polarity_scores(review)[\"compound\"]\n",
    "\n",
    "    # Determina se la recensione è positiva o negativa in base al valore del sentiment\n",
    "    if sentiment > 0:\n",
    "        return \"positive\"\n",
    "    elif sentiment < 0:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Registra la funzione come UDF (User Defined Function)\n",
    "sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# Applica la sentiment analysis al DataFrame\n",
    "classified_df = spark_df.withColumn(\"sentiment\", sentiment_udf(spark_df[\"review\"]))\n",
    "\n",
    "# Dividi il DataFrame in due DataFrame separati per le recensioni positive e negative\n",
    "positive_reviews = classified_df.filter(classified_df[\"sentiment\"] == \"positive\")\n",
    "negative_reviews = classified_df.filter(classified_df[\"sentiment\"] == \"negative\")\n",
    "\n",
    "\n",
    "# Mostra i risultati del DataFrame positive_reviews come DataFrame Pandas\n",
    "positive_reviews_pandas = positive_reviews.toPandas().head(5)\n",
    "print(\"Positive Reviews:\")\n",
    "print(positive_reviews_pandas)\n",
    "\n",
    "# Mostra i risultati del DataFrame negative_reviews come DataFrame Pandas\n",
    "negative_reviews_pandas = negative_reviews.toPandas().head(5)\n",
    "print(\"Negative Reviews:\")\n",
    "print(negative_reviews_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1687268761506,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "a5nZGEa4QWLq",
    "is_executing": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RBr0SO0QYHa"
   },
   "source": [
    "# ciao prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1687268761506,
     "user": {
      "displayName": "Ilaria De Sio",
      "userId": "04931593397008720475"
     },
     "user_tz": -120
    },
    "id": "HlfTQSO8SnPs",
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Weather_forecasting_pyspark",
   "notebookOrigID": 1252952709241176,
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "mw8uGOrDggaB",
    "Hr5wlq6mggaF",
    "CTT3vraEggaF",
    "CrLlA6yhAuYF",
    "FGXW-f_I6lS4",
    "ZjZK02GF-KMg",
    "kFrIr8v5wVH6"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
